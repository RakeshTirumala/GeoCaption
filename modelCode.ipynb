{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1ZUkPY71Pym64ebS4l17QGq_syZsDaoip","authorship_tag":"ABX9TyNG5uLBNG4TeL7i8+MQYMoJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Yq7IVEXdsya-"},"outputs":[],"source":["# !pip install datasets\n","!pip install tensorflow-gpu==2.9.0"]},{"cell_type":"code","source":["size = 8736"],"metadata":{"id":"7gQq2BZQzpXW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","features = pickle.load(open(\"/content/drive/MyDrive/MajorProject/features.pkl\", \"rb\"))\n","# features = list(features.values())\n","features = features[:size]\n","print(\"Completed!\")\n","print(len(features))"],"metadata":{"id":"h3G-n9gYhC5o"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c5NMU6f82tfx"},"outputs":[],"source":["import random\n","import numpy as np\n","# from datasets import load_dataset\n","import warnings\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import cv2\n","from PIL import Image\n","from tensorflow.keras.utils import Sequence\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n","import re\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras import Input\n","from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n","from keras.layers import Reshape, concatenate, add\n","from keras.layers import Dense, Dropout\n","from keras.utils.np_utils import to_categorical\n","from tqdm import tqdm\n","from tensorflow.keras.applications import DenseNet201\n","from keras.models import Sequential, Model\n","from keras.layers import Embedding, LSTM, GRU\n","import tensorflow"]},{"cell_type":"code","source":["# plt.rcParams['font.size'] = 12\n","# sns.set_style(\"dark\")\n","# warnings.filterwarnings('ignore')"],"metadata":{"id":"wSBQxEjz3esK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def text_preprocessing(dataCap):\n","    for index in range(len(dataCap)):\n","        caption = dataCap[index]\n","        caption = caption.lower()\n","        # caption = re.sub(re.compile('[^A-Za-z]'), '', caption)\n","        # caption = \" \".join(caption.split())\n","        # caption = re.sub(r\"(?<![a-zA-Z])[a-zA-Z](?![a-zA-Z])\", \"\", caption)\n","        caption = \"startseq \" + caption + \" endseq\"\n","        dataCap[index] = caption\n","    return dataCap\n","\n","# LOADING THE DATASET\n","# dataset = load_dataset(\"arampacha/rsicd\")\n","# print(dataset)\n","\n","# # ARRANGING THE DATA\n","# print(\"STARTING ARRANGEMENT...\")\n","# dataImage = dataset[\"train\"][\"image\"] + dataset[\"test\"][\"image\"] + dataset[\"valid\"][\"image\"]\n","# dataCap = dataset[\"train\"][\"captions\"] + dataset[\"test\"][\"captions\"] + dataset[\"valid\"][\"captions\"]\n","# dataFilename = dataset[\"train\"][\"filename\"] + dataset[\"test\"][\"filename\"] + dataset[\"valid\"][\"filename\"]\n","\n","dataImage = pickle.load(open(\"/content/drive/MyDrive/MajorProject/dataImages.pkl\", \"rb\"))\n","dataCap = pickle.load(open(\"/content/drive/MyDrive/MajorProject/dataCaptions.pkl\", \"rb\"))\n","dataFilename = pickle.load(open(\"/content/drive/MyDrive/MajorProject/dataFilename.pkl\", \"rb\"))\n","dataImage = dataImage[:size]\n","dataCap = dataCap[:size]\n","dataFilename = dataFilename[:size]\n","\n","for idx in range(len(dataCap)):\n","    dataCap[idx] = dataCap[idx]\n","print(\"Images length: \", len(dataImage))\n","print(\"Captions length: \", len(dataCap))\n","dataCap = text_preprocessing(dataCap)\n","print('\\n'.join(map(str, dataCap[:5])))\n","\n","# tokenizer = Tokenizer()\n","# tokenizer.fit_on_texts(dataCap)\n","# vocab_size = len(tokenizer.word_index) + 1\n","# max_length = max(len(caption.split()) for caption in dataCap)\n","\n","# ndataImages = len(dataImage)\n","# split_index = round(0.70 * ndataImages)\n","# train = dataImage[:split_index]\n","# test = dataImage[split_index:]\n","# trainCaps = dataCap[:split_index]\n","# testCaps = dataCap[split_index:]\n","\n","\n","# model = DenseNet201()\n","# fe = Model(inputs=model.input, outputs=model.layers[-2].output)\n","# features = {}\n","# for imageO in tqdm(dataImage):\n","#     img = np.array(imageO)\n","#     img = img.reshape((224,224,3))\n","#     img = img / 255.0\n","#     img = np.expand_dims(img, axis=0)\n","#     feature = fe.predict(img, verbose=0)\n","#     idx = dataImage.index(imageO)\n","#     filename = dataFilename[idx]\n","#     features[filename] = feature"],"metadata":{"id":"hL3xjexAjZeh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pickle.dump(dataImage, open(\"/content/drive/MyDrive/MajorProject\", \"wb\"))"],"metadata":{"id":"YW26R9FDkRGj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(dataCap)\n","vocab_size = len(tokenizer.word_index) + 1\n","max_length = max(len(caption.split()) for caption in dataCap)\n","\n","ndataImages = len(dataImage)\n","split_index = round(0.85 * ndataImages)\n","train = dataImage[:split_index]\n","test = dataImage[split_index:]\n","trainCaps = dataCap[:split_index]\n","testCaps = dataCap[split_index:]\n","\n","\n","model = DenseNet201()\n","fe = Model(inputs=model.input, outputs=model.layers[-2].output)"],"metadata":{"id":"fl9Dj0pAjyVO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CustomDataGenerator(Sequence):\n","    def __init__(self, X, y, batch_size, tokenizer, dataFilename,\n","                  vocab_size, max_length, features, shuffle=False):\n","\n","        self.X = X.copy()\n","        self.y = y.copy()\n","        self.batch_size = batch_size\n","        self.tokenizer = tokenizer\n","        self.vocab_size = vocab_size\n","        self.max_length = max_length\n","        self.features = features\n","        self.dataFilename = dataFilename\n","        self.shuffle = shuffle\n","        self.n = len(self.X)\n","\n","    def on_epoch_end(self):\n","        if self.shuffle:\n","            zipped = list(zip(self.X, self.y))\n","            random.shuffle(zipped)\n","            self.X, self.y = zip(*zipped)\n","\n","    def __len__(self):\n","        return self.n // self.batch_size\n","\n","    def __getitem__(self, index):\n","\n","        X1, X2, y = self.__get_data(index)\n","        return (X1, X2), y\n","\n","    def __get_data(self, index):\n","\n","        X1, X2, y = list(), list(), list()\n","        start_index = index * self.batch_size\n","        end_index = start_index + self.batch_size\n","\n","        images = self.X[start_index:end_index]\n","        captions = self.y[start_index:end_index]\n","        length = len(images)\n","        for idx in range(length):\n","            image = images[idx]\n","            # fn= dataFilename[idx]\n","            feature = self.features[idx][0]\n","            caption = captions[idx]\n","            # captions = self.y[start_index:end_index]\n","            # for caption in captions:\n","            seq = self.tokenizer.texts_to_sequences([caption])[0]\n","\n","            for i in range(1, len(seq)):\n","                in_seq, out_seq = seq[:i], seq[i]\n","                in_seq = pad_sequences([in_seq], maxlen=self.max_length)[0]\n","                out_seq = to_categorical([out_seq], num_classes=self.vocab_size)[0]\n","                X1.append(feature)\n","                X2.append(in_seq)\n","                y.append(out_seq)\n","\n","        X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n","\n","        return X1, X2, y"],"metadata":{"id":"umipaK8V8GPm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def text_preprocessing(dataCap):\n","#     for index in range(len(dataCap)):\n","#         caption = dataCap[index]\n","#         caption = caption.lower()\n","#         # caption = re.sub(re.compile('[^A-Za-z]'), '', caption)\n","#         # caption = \" \".join(caption.split())\n","#         # caption = re.sub(r\"(?<![a-zA-Z])[a-zA-Z](?![a-zA-Z])\", \"\", caption)\n","#         caption = \"startseq \" + caption + \" endseq\"\n","#         dataCap[index] = caption\n","#     return dataCap\n","\n","# # LOADING THE DATASET\n","# dataset = load_dataset(\"arampacha/rsicd\")\n","# print(dataset)\n","\n","# # ARRANGING THE DATA\n","# print(\"STARTING ARRANGEMENT...\")\n","# dataImage = dataset[\"train\"][\"image\"] + dataset[\"test\"][\"image\"] + dataset[\"valid\"][\"image\"]\n","# dataCap = dataset[\"train\"][\"captions\"] + dataset[\"test\"][\"captions\"] + dataset[\"valid\"][\"captions\"]\n","# dataFilename = dataset[\"train\"][\"filename\"] + dataset[\"test\"][\"filename\"] + dataset[\"valid\"][\"filename\"]\n","# for idx in range(len(dataCap)):\n","#     dataCap[idx] = dataCap[idx][1]\n","# print(\"Images length: \", len(dataImage))\n","# print(\"Captions length: \", len(dataCap))\n","# dataCap = text_preprocessing(dataCap)\n","# print('\\n'.join(map(str, dataCap[:5])))\n","\n","# tokenizer = Tokenizer()\n","# tokenizer.fit_on_texts(dataCap)\n","# vocab_size = len(tokenizer.word_index) + 1\n","# max_length = max(len(caption.split()) for caption in dataCap)\n","\n","# ndataImages = len(dataImage)\n","# split_index = round(0.70 * ndataImages)\n","# train = dataImage[:split_index]\n","# test = dataImage[split_index:]\n","# trainCaps = dataCap[:split_index]\n","# testCaps = dataCap[split_index:]\n","\n","\n","# model = DenseNet201()\n","# fe = Model(inputs=model.input, outputs=model.layers[-2].output)\n","# features = {}\n","# for imageO in tqdm(dataImage):\n","#     img = np.array(imageO)\n","#     img = img.reshape((224,224,3))\n","#     img = img / 255.0\n","#     img = np.expand_dims(img, axis=0)\n","#     feature = fe.predict(img, verbose=0)\n","#     idx = dataImage.index(imageO)\n","#     filename = dataFilename[idx]\n","#     features[filename] = feature"],"metadata":{"id":"fnxCdlJd8XPa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import pickle\n","# with open('/content/drive/MyDrive/MajorProject/features.pkl', 'wb') as f:\n","#     pickle.dump(features, f)"],"metadata":{"id":"wA0m3VMns3M4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.optimizers import Adam\n","\n","input1 = Input(shape=(1920,))\n","input2 = Input(shape=(max_length,))\n","img_features = Dense(1024, activation='relu')(input1)\n","img_features_reshaped = Reshape((1, 1024), input_shape=(1024,))(img_features)\n","sentence_features = Embedding(vocab_size, 1024, mask_zero=False)(input2)\n","merged = concatenate([img_features_reshaped, sentence_features], axis=1)\n","sentence_features = LSTM(1024)(merged)\n","x = Dropout(0.25)(sentence_features)\n","x = add([x, img_features])\n","x = Dense(1024, activation='relu')(x)\n","x = Dropout(0.25)(x)\n","output = Dense(vocab_size, activation='softmax')(x)\n","\n","caption_model = Model(inputs=[input1, input2], outputs=output)\n","model_optimizer = Adam(learning_rate=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n","caption_model.compile(loss='categorical_crossentropy', optimizer=model_optimizer,metrics = ['accuracy'])\n","caption_model.summary()\n","\n","train_generator = CustomDataGenerator(train, trainCaps, batch_size=64, tokenizer=tokenizer,\n","                                      dataFilename=dataFilename[:split_index],\n","                                      vocab_size=vocab_size, max_length=max_length, features=features)\n","\n","validation_generator = CustomDataGenerator(test, testCaps, batch_size=64, tokenizer=tokenizer,\n","                                            dataFilename=dataFilename[split_index:],\n","                                            vocab_size=vocab_size, max_length=max_length, features=features)\n","print(train_generator)\n","print(validation_generator)"],"metadata":{"id":"AJYAayib_PYL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model_name = \"ImageCaptioningmodel_100_V1.h5\"\n","# checkpoint = ModelCheckpoint(model_name,\n","#                               monitor=\"val_loss\",\n","#                               mode=\"min\",\n","#                               save_best_only=True,\n","#                               verbose=1)\n","# earlystopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=5,\n","#                               verbose=1, restore_best_weights=True)\n","\n","# learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss',\n","#                                             patience=3,\n","#                                             verbose=1,\n","#                                             factor=0.2)\n","\n","history = caption_model.fit(\n","    train_generator,\n","    epochs=25,\n","    validation_data=validation_generator)\n","caption_model.save('/content/drive/MyDrive/MajorProject/ImageCaptioningmodel_100_V1.h5')\n","print(history.history.keys())"],"metadata":{"id":"1LbokHIFXp7w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test = pickle.load(open(\"/content/drive/MyDrive/MajorProject/dataImages.pkl\", \"rb\"))\n","test = test[size:]\n","print(len(test))"],"metadata":{"id":"AE1nUWZ_tI8v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.models import load_model\n","# img_path = \"/content/drive/MyDrive/MajorProject/church_015.jpg\"\n","def idx_to_word(integer,tokenizer):\n","    \n","    for word, index in tokenizer.word_index.items():\n","        if index==integer:\n","            return word\n","    return None\n","\n","\n","\n","def predict_caption(model, img, tokenizer, max_length, features):\n","    \n","    feature = fe.predict(img, verbose=0)\n","    # for fv in features.values():\n","    #   if feature == fv:\n","    #     break\n","    #   res = feature0\n","\n","    in_text = \"startseq\"\n","    for i in range(max_length):\n","        sequence = tokenizer.texts_to_sequences([in_text])[0]\n","        sequence = pad_sequences([sequence], max_length)\n","\n","        y_pred = model.predict([feature,sequence])\n","        y_pred = np.argmax(y_pred)\n","        \n","        word = idx_to_word(y_pred, tokenizer)\n","        \n","        if word is None:\n","            break\n","            \n","        in_text+= \" \" + word\n","        \n","        if word == 'endseq':\n","            break\n","            \n","    return in_text \n","\n","# img = load_img(img_path)\n","\n","# # img = img_to_array(img)\n","# # img = np.array(img)\n","# img = img_to_array(img)\n","\n","# img = load_img(img_path)\n","img = np.array(test[2000])\n","# img = img_to_array(img)\n","img = cv2.resize(img,(224,224))\n","img = np.expand_dims(img, axis=0)\n","\n","saved_model = load_model('/content/drive/MyDrive/MajorProject/ImageCaptioningmodel_100_V1.h5')\n","with open('/content/drive/MyDrive/MajorProject/features.pkl', 'rb') as f:\n","    featuresS = pickle.load(f)\n","caption = predict_caption(saved_model, img, tokenizer, max_length, featuresS)\n","caption = caption.replace(\"startseq\", \"\")\n","caption = caption.replace(\"endseq\",\"\")\n","print(caption)"],"metadata":{"id":"7y6s2uFJFFxQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","img = np.array(test[2000])\n","plt.imshow(img)\n","plt.show()"],"metadata":{"id":"cZcgpK5WIdjn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('Model Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.savefig(\"/content/drive/MyDrive/MajorProject/ImageCaptioningmodel_100_V1.png\")\n","plt.show()"],"metadata":{"id":"jjWhFLeehT26"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"TTxhZQxPsJU7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KWT2CluUsRwN"},"execution_count":null,"outputs":[]}]}